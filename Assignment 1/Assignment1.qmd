---
title: "Assignment 1"
author: "David Gyaraki, Thao Le"
format: pdf
pdf:
  documentclass: article
  cite-method: biblatex
editor: visual
include-in-header:
  text: |
    \addtokomafont{disposition}{\rmfamily}
    \usepackage{amsmath}
    \newcommand{\bm}{\symbf}
    \newcommand{\T}{\text{T}}
    \newcommand{\pl}{\text{plim}}
    \usepackage{fancyvrb}
pdf-engine: xelatex
cap-location: top
toc: true
toc-title: Contents
number-sections: true
mainfont: Helvetica
setspace:
  linestretch: 1.25
fig-align: center
table-align: center
fig-pos: H
table-pos: H
execute:
  echo: true
  warning: false
  eval: true
code-line-numbers: false
colorlinks: true
code-block-bg: darkgray
df-print: default
highlight-style: arrow-dark
biblio-title: References
---

\clearpage

```{r setup}
# load packages
if(!require(pacman)){install.packages("pacman")}

p_load(devtools,tidyverse,dplyr,ggplot2,latex2exp,
       cowplot,tseries,sampleSelection)

#load data
dfData = read.csv("assignment1_2023.csv")
attach(dfData) 
```

```{=tex}
\section{Question 1}
\subsection{(i)}
```
```{r}
lm_model = lm(logwage ~ age + agesq + schooling, data = dfData)
summary(lm_model)
```
Looking at our OLS results, firstly, our F-statistic is significant, which means that there is an association between at least of one of the predictor variables and logwage. Thus, we can move on to interpret the coefficients. The adjusted R-square of 0.8135 means that $81.35\%$ of the variance in the dependent variable can be explained by the model. Looking at our OLS estimate, only the OLS estimate of schooling is significant. Thus, we can only interpret the effect of the variable schooling. There is an association between the years of schooling and the salary of a person. Holding other variables constant, a year of schooling is associated with around 0.2160 units increase in log salary of an individual.
\subsection{(ii)}

The sample selection problem here is to choose observations of the non-employed, which are those who have no income. The selection equation is then: \begin{equation*}
    I_i = 
    \begin{cases}
    1 \text{ if logwage} > 0\\
    0 \text{ otherwise},
    \end{cases} 
\end{equation*} and the second regression equation is: \begin{equation*}
    Y_i^* = \bm{X_i'}\bm{\beta} + U_i.
\end{equation*} We select a sample consisting of: \begin{equation*}
    Y_i = 
    \begin{cases}
    Y_i^* \text{ if } I_i = 1\\
    \text{missing} \text{ if } I_i = 0,
    \end{cases} 
\end{equation*}

An OLS may fail in this context because the dependent variable (logwage) is missing for the non-employed sample, thus, it is not possible to derive an estimate of this variable for the non-employed

\subsection{(iii)}

The exclusion restriction variable is one that is included in $\bm{Z_i}$ but excluded from $\bm{X_i}$, I would choose 'married' as a suitable candidate for the sample selection model. My motivation is that married people tends to have stable income, and thus, employed.

```{r Heckman}
# Create I variable:
dfData = mutate(dfData, vI = if_else(logwage > 0, TRUE, FALSE)) 
dfData["vI"][is.na(dfData["vI"])] <- FALSE

# Heckman model with restriction
heckman_rest = heckit( vI ~ married+age + agesq + 
                      schooling, logwage ~ age + agesq + 
                      schooling, data = dfData)
summary(heckman_rest)
```

```{r Heckman 2}
# Heckman model without restriction
heckman_unrest = heckit( vI ~ married + age + agesq + 
                           schooling, logwage ~ age + agesq 
                         + schooling + married, data = dfData)
summary(heckman_unrest)
```

Looking at the outcomes of the two model, we can see that the unrestricted model have a much higher standard error, this is because the unrestricted model run into the problem of multicollinearity (i.e., the Inverse Mill Ratio is almost perfectly colliniear to the rest of the explanatory variables). Because of multicolinearity, the variable schooling is no longer statistically significant in the model. Thus, we are more unsure of our estimates.

\subsection{(iv)}

```{r ML}
# Maximum likelihood estimator, restricted
ML_rest = selection(vI ~ married+age + agesq + schooling, 
                    logwage ~ age + agesq + schooling, data = dfData)
summary(ML_rest)

# Maximum likelihood estimator, unrestricted
ML_unrest = selection(vI ~ married + age + agesq + schooling, 
                      logwage ~ age + agesq + schooling + married, 
                      data = dfData)
summary(ML_unrest)

```

Similar to the situation in (iii), we can see that the unrestricted model have a much higher standard error, this is because the unrestricted model run into the problem of multicollinearity (i.e., the Inverse Mill Ratio is almost perfectly colliniear to the rest of the explanatory variables). Because of in increase in standard errors, the variable schooling is no longer statistically significant in the model. Thus, we are unsure of our estimates.

\subsection{(v)}

To specify the distribution of potential earnings for the non-employed, we first get a subsample of the unemployed individuals. Then, we use one of the restricted models in (iii) or (iv) to predict potential income of the non-employed and draw a histogram.
```{r }
# Get subsample of unemployed individuals
dfUnEmployed = dfData[dfData$vI == FALSE, ]

predicted_income = predict(ML_rest, newdata = dfUnEmployed)
hist(predicted_income)
```

The histogram does not give an apparent normal distribution. However, we can say that most predictions lies between 6 and 8, and the distribution is slightly left-skewed.

```{=tex}
\section{Question 2}
\subsection{(i)}
```
```{r schooling}
# Get subsample of employed individuals
dfEmployed = dfData[dfData$vI == TRUE, ]

model0 = lm(logwage ~ schooling + age + agesq, data = dfEmployed)
summary(model0)
```


Looking at our OLS results, firstly, our F-statistic is significant, which means that there is an association between at least of one of the predictor variables and logwage. Thus, we can move on to interpret the coefficients. The OLS estimate of schooling is significant. Thus, we can only interpret the effect of the variable schooling. There is an \emph{association} between the years of schooling and the salary of a person. Holding other variables constant, a year of schooling is associated with around 0.2160 increase in log salary of an individual.

However, we CANNOT discus the causal effect of schooling on income, because association is different from causation.

Regarding whether it is plausible that regularity conditions for applying OLS are satisfied. We believe it is plausible that some conditions such as homoskedasticity, X non-random, the error terms are normally distributed and has mean zero, no-auto correlation are satisfied, the condition that model is linear is also satisfied. However we are not sure if there is any multicollinearity between the explanatory variable yet. For example, the distance to school and the regional subsidy for school expenses might be correlated with the years of schooling of an individual.
\subsection{(ii)}

```{r }
# Using distance as instrument variable
model1 = lm(schooling ~ distance, data = dfEmployed)
X.hat.1 = fitted.values(model1)

# Fit Linear regression model again using the fitted values of first step
model2 =lm(logwage ~ X.hat.1 + age + agesq, data = dfEmployed)
summary(model2)
```

```{r }
# Using subsidy as instrument variable
model3 = lm(schooling ~ subsidy , data = dfEmployed)
X.hat.3 = fitted.values(model3)

# Fit Linear regression model again using the fitted values of first step

model4 =lm(logwage ~ X.hat.3 + age + agesq , data = dfEmployed)
summary(model4)
```

```{r }
# Using subsidy and distance as instrument variable
model5 = lm(schooling ~ subsidy+distance, data = dfEmployed)
X.hat.5 = fitted.values(model3)

model6 =lm(logwage ~ X.hat.5 + age + agesq, data = dfEmployed)
summary(model6)
```
After using 3 options of instrument variables, we can see that 'distance' is not a good instrument variable as the part of 'schooling' not explained by 'distance' (stored in 'X.hat.1') is not statistically significant in the linear model. On the other hand, after adding 'subsidy' as an instrument variable, the part of 'schooling' not correlated with 'subsidy' is statistically significant in explaining the changes in 'logwage'. We will only use 'subsidy' as the instrument variable, not a combination of both 'subsidy' and 'schooling', because using both variables can lead to the issue of over-identification.
\subsection{(iii)}

```{r}
# OLS outcomes
model0$coefficients
```
```{r}
# IV outcomes
model4$coefficients
```
Above are the OLS and IV estimates, in which, X.hat.3 in the IV estimates is the part of 'schooling' that is not correlated with subsidy. 

Looking at the outcomes, we can see that the IV estimate of schooling coefficient is higher than the OLS estimate. This might be because the effect of subsidy on schooling is eliminated, making the number of years of schooling smaller, and thus it needs higher weights to predict the income. Both OLS and IV estimates yielded significant effect from the schooling variable.

We would prefer OLS in the case where there is no correlation between explanatory variables and the error terms.

To decide between OLS and IV, we first perform a t-test to check the relevance of the instrument (i.e., checking whether 'subsidy' and 'schooling' are correlated).

```{r}
summary(model3)
```
We can see that there is statistically sigfificant evidence that there is an association between 'subsidy' and 'schooling'. Thus, 'subsidy' is a relevant instrument. Moreover, we also need to check the validity of the instrument using the Sargan test:
```{r}
sargan_test = lm(model3$residuals ~ subsidy + age + agesq, data = dfEmployed)

test_statistics <- summary(sargan_test)$r.squared*nrow(dfEmployed)

print(1-pchisq(test_statistics,1))  # prints p-value
```
We can see that the p-value is 0.0167793, which is significant at $\alpha = 0.05$. This means that our instrument variable is valid. 

Using the result of both the t-test and the Sargan test, we choose to use the IV estimate instead of the OLS one as the IV estimate utilises a relevant and valid instrument variable.